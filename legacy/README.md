# Demestihas-AI

Custom AI agent with branched orchestration, dual-memory system, and commercial parity performance.

## ðŸš€ Features

- **Branched Orchestration** - Fast path for casual chat (<100ms latency)
- **Summary Buffer Memory** - 60-80% token usage reduction
- **Lightweight Router** - Intent classification with gpt-4o-mini
- **Dual Memory System** - FalkorDB knowledge graph + PostgreSQL conversations
- **Hybrid Execution** - Internal agents + external Arcade tools
- **Reflection Loop** - Judge LLM critique for low-confidence decisions
- **ReAct Pattern** - Iterative reasoning with tool observations

## ðŸ“‹ Architecture

```
User Query â†’ Pre-Router (Intent Classification)
              â”œâ”€ CASUAL_CHAT â†’ Fast Path â†’ Response (âš¡ <100ms)
              â””â”€ COMPLEX_TASK/KNOWLEDGE_QUERY â†’ Full LangGraph
                  â”œâ”€ Reflection Loop (if confidence < 0.6)
                  â”œâ”€ Hybrid Executor (agents + tools)
                  â”œâ”€ ReAct Loop (iterative reasoning)
                  â””â”€ Knowledge Consolidation â†’ Response
```

## ðŸ› ï¸ Tech Stack

- **Framework:** LangGraph, FastAPI
- **LLMs:** OpenAI (GPT-4, GPT-4o-mini)
- **Memory:** FalkorDB (graph), PostgreSQL (conversations), Qdrant (vectors)
- **Tools:** Arcade API integration
- **Deployment:** Docker, Docker Compose
- **UI:** Streamlit

## ðŸš¦ Quick Start

### Prerequisites

- Docker & Docker Compose
- OpenAI API key
- Python 3.10+

### Installation

1. **Clone the repository:**

   ```bash
   git clone https://github.com/YOUR_USERNAME/demestihas-ai.git
   cd demestihas-ai
   ```

2. **Configure environment:**

   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

3. **Start services:**

   ```bash
   docker-compose up -d
   ```

4. **Access the UI:**
   - Streamlit: <http://localhost:8501>
   - API: <http://localhost:8000>
   - Health check: <http://localhost:8000/health>

## ðŸ“¦ Project Structure

```
demestihas-ai/
â”œâ”€â”€ agent/                  # Main agent service
â”‚   â”œâ”€â”€ main.py            # FastAPI app + LangGraph orchestrator
â”‚   â”œâ”€â”€ statefulness_extensions.py  # Memory management
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ streamlit/             # Streamlit UI
â”œâ”€â”€ postgres/              # PostgreSQL init scripts
â”œâ”€â”€ docker-compose.yml     # Service orchestration
â””â”€â”€ .env                   # Environment variables (not committed)
```

## ðŸ“Š Performance

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Casual Chat Latency | 3-5s | <100ms | **97% faster** |
| Token Usage (Long Conversations) | 5000+ | 1000-2000 | **60-80% reduction** |
| Intent Classification Accuracy | N/A | 95%+ | **New capability** |

## ðŸš€ Deployment

See [deployment_guide.md](deployment_guide.md) for VPS deployment instructions.

**Quick Deploy:**

```bash
./deploy.sh
```

## ðŸ·ï¸ Releases

### v1.0 - Commercial Parity (Current)

- âœ… Branched orchestration with fast path
- âœ… Summary buffer memory
- âœ… Lightweight router with intent classification
- âœ… 97% latency reduction for casual chat
- âœ… 60-80% token usage reduction

---

**Built with â¤ï¸ for intelligent, efficient AI interactions**
